<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: big_data | Puncsky CS Notebook]]></title>
  <link href="http://www.puncsky.com/blog/categories/big-data/atom.xml" rel="self"/>
  <link href="http://www.puncsky.com/"/>
  <updated>2016-02-13T11:14:44-08:00</updated>
  <id>http://www.puncsky.com/</id>
  <author>
    <name><![CDATA[Tian]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[面向痛苦编程 (译文)]]></title>
    <link href="http://www.puncsky.com/blog/2015/05/06/suffering-oriented-programming/"/>
    <updated>2015-05-06T18:04:00-07:00</updated>
    <id>http://www.puncsky.com/blog/2015/05/06/suffering-oriented-programming</id>
    <content type="html"><![CDATA[<p>Nathan Marz 是 Twitter 的分布式流处理框架 Storm 的主要作者，他的新书 <em>Big Data: Principles and best practices of scalable realtime data systems</em> 今天正式<a href="http://www.amazon.com/dp/1617290343">发售</a>了，我半个月前就下了订单，满心期盼。先前在浏览他的<a href="http://nathanmarz.com/">博客</a>时看到一篇有趣的文章，发表于2012年，现中文翻译于此，和大家分享。</p>

<hr />

<p>面向痛苦编程</p>

<p>作者：Nathan Marz</p>

<p>译者：潘天 (puncsky)</p>

<p>前些天，有人问了我个有趣的问题：“在创业公司工作的时候，还能够有勇气去写 <a href="http://storm.apache.org/">Storm</a>，你是怎样做到的？” 我知道，在外界看来，创业公司做如此大规模的项目，是相当冒险的行为。而在我自己看来，其实，写 Storm 一点儿风险都没有：它很有挑战性，但并没有风险。</p>

<p>我遵从了一种能够极大地规避风险的开发方式，来应对像 Storm 这种大型项目，并将其命名为 “面向痛苦编程” 。简单来说，面向痛苦编程是这样的：不去做那些不重要的项目，除非，缺少它真的让你感到痛苦难耐。这一准则放之四海而皆准，大到架构上的决策，小到每天写的代码。面向痛苦编程保证你总是在做重要的事情，在尝试大的投入前先从小处精通问题的本质，因此而能够极大地规避风险。</p>

<p>面向痛苦编程有三句箴言：“先做成，再做美，后做快。” (First make it possible. Then make it beautiful. Then make it fast.)</p>

<h2>先做成</h2>

<p>当你进入某个你不熟悉的问题领域 (problem domain) 时，不要立马写一个 “通用的” 或者 “可拓展” 的解。你都不太了解，如何预见未来会有哪些需求？你会把不必通用的变通用，浪费了时间，增加了复杂性。</p>

<p>更好的做法是仅仅 “三下五除二干出来再说” (hack things out) ，直面解决手头的问题。这样，你就能够做成你需要做成的事儿，避免浪费时间。</p>

<p>Storm 的 “先做成” 阶段是一年的快速开发，用队列和工作节点做一个流试处理处理系统。我们学到了如何用 “ack” 协议保证数据处理，我们学到了如何用集群的队列和工作节点规模化实时计算，我们学到了有时候要用不同的方式分割 (partition) 消息流：有时候随机地、有时候 hash/mod 地，确保同样的东西总是流到同样的工作节点上。</p>

<p>当时我们还甚至不知道我们处在 “先做成” 的阶段，只是专注于做产品。尽管队列和工作节点们形成的系统在后来很快让我们痛得不行，但是当时，规模化这个系统很乏味，我们也并不需要容错机制。很明显，队列和工作节点形成的范式并不是一层正确的抽象，因为我们大多数的代码与路由 (routing messages) 和序列化有关，并非我们真正关心的业务逻辑。</p>

<p>同时，开发产品让我们发现了 “实时计算” 的新用例。我们写了一个功能，计算 Twitter 上某一个 URL 的 受众量 (Reach)，所谓受众量，就是能够接触到 Twitter 上的某个 URL 的去除重复后的人数。这一计算很困难，一次计算就需要上百次的数据库访问、和上千万的去重操作。要处理这些绝对路径的 URL，我们最初的单机实现要跑一分多钟，显然，我们需要某种分布式系统来并行处理，加速计算。</p>

<p>启发 Storm 的一个关键就在于，我们意识到了 “受众量问题” 和 “流处理问题” 可以被归化成简单的抽象。</p>

<h2>再做美</h2>

<p>当你三下五除二干出来再说 (hacking things out) 的时候，这一问题领域的 “地图” 也逐渐明晰起来。随着时间的推移，你逐渐习得这一领域越来越多的用例，深入了解这些系统的微妙与复杂之处。这些深入的了解能够启发 “美丽” 的技术，来替代现有的系统，减轻你的痛苦，让过去的不可能做到的新系统、新功能变成可能。</p>

<p>做“美”的关键在于，搞清楚能够解决已知具体问题的最简抽象集。不要预测那些实际上你并未遇到的具体用例，否则结果就会做得太过了 (overengineering)。经验法则是，要想投入更多，对问题领域的理解需要更深刻，用例需要更多样。否则，就有发生<a href="http://en.wikipedia.org/wiki/Second-system_effect">第二系统效应</a>的风险。</p>

<p>在“做美”阶段，你动用设计与抽象的技能，把问题区间提炼成简单的、可组合在一起的抽象们。提炼美好的抽象就似统计学所谓的回归一般：图上一堆点 (用例) ，找条最简单的曲线 (抽象) 契合这些点。</p>

<p>用例越多，找契合点的曲线也就越方便。如果点不够，得到的曲线要么做得太过，要么做得不够好，最终过度工程或者浪费时间。</p>

<p>做美有很大一块是去了解问题区间的性能与资源特征 (understanding the performance and resource characteristics of the problem space)，设计优美的解要利用那些属于前一阶段“做成”时学到的微妙与复杂。</p>

<p>就 Storm 而言，我把实时处理提炼成了一堆小的抽象：流 (streams)，出水口 (spouts)，水栓 (bolts)，和拓扑结构 (topologies)。我发明了新的算法，消除中间消息的代理，同时又能够保证数据得到了处理，整个系统中最复杂最令人痛苦的部分因此而被除去。流处理和受众量，两个看上去迥异的问题，如此优雅地归化到了 Storm，这强烈地预示着，我做的东西，了不得。</p>

<p>我继而进一步地为 Storm 寻找更多的用例，验证我的设计。我和其他工程师深入讨论，学习他们处理实时问题的独到之处。我并没有只去问我认识的人，我还发了 Twitter 说我正在做一款新的实时系统，想要知道其他人的用例，随后有很多有趣的讨论，让我对问题领域有了更深的认识，并验证了我的设计思路。</p>

<h2>后做快</h2>

<p>一旦得到了优美的设计，就能安全地花时间做性能分析 (profiling) 和优化了。过早优化浪费时间，因为你仍然有可能重新做设计。</p>

<p>“做快”并不是指系统高层级的性能特征。这些特征本应该在“做成”阶段被理解到，在“做美”阶段被设计好。“做快”是指微观上的优化，让代码更紧凑，更有效率地利用资源。所以说，在“做美”阶段，你会关心诸如渐进复杂度的问题；在“做快”阶段，你会关心诸如复杂度中那些常量的问题。</p>

<h2>反复雕琢</h2>

<p>面向痛苦编程是持续的过程。构建美丽的系统赋予你新的能力，让你在问题区间里更新更深的领域有能力“做成”。学到的新的知识又反馈到原有的技术中，用以微调或者补充原有的抽象，这样就能够搞定更多的用例。</p>

<p>Storm 就像这样迭代了好多次。一开始使用 Storm 的时候，我们发现了，单一的组件需要有能力放出多个独立的流。我们发现了，Storm 批处理元组 (process batches of tuples) 作为具体的单元，需要增加一种特殊的“直接流 (direct stream) ”。最近，我又开发了“事务性拓扑 (transactional topologies)” ，在 Storm 至少处理一次的基础上，对几乎任意情况的实时处理需求，达成恰好通知一次的语义。</p>

<p>当然了，在自己不了解的领域三下五除二干出来再说，然后持续性地迭代，会导致一些渣代码。而面向痛苦编程最重要的特性便是大无畏地专注于重构。这正是规避“<a href="http://en.wikipedia.org/wiki/Accidental_complexity">偶发复杂度 (accidental complexity)</a>” (译者注：将概念上的构思施行于电脑上所遭遇到的困难。) 破坏项目的核心所在。</p>

<h2>结论</h2>

<p>用例在面向痛苦编程中至关重要，用黄金来形容它们的价值也不为过。而习得用例的唯一方法，就是写代码积累经验。</p>

<p>绝大多数的程序员都会经历这样的进化过程。一开始挣扎地让东西跑起来，代码乱得毫无章法，渣代码和拷贝粘贴的代码一塌糊涂。后来，你会发现结构化编程的好处，尽可能多地共享代码逻辑。然后，你学到了如何抽象，用封装的思想看系统。再后来，你着迷于通用化你的代码，让一切的一切可以拓展到未来的程序中。</p>

<p>面向痛苦编程拒绝相信人们能够有效地预测未知的需求，它承认，在不了解问题领域的情况下通用化代码，终将导致复杂与浪费。设计一定要，也总是要，驱动于活生生的用例。</p>

<p>EOF</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[H-Quincy: Fair Scheduling for Hadoop Clusters]]></title>
    <link href="http://www.puncsky.com/blog/2013/09/14/h-quincy/"/>
    <updated>2013-09-14T16:49:00-07:00</updated>
    <id>http://www.puncsky.com/blog/2013/09/14/h-quincy</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/puncsky/H-Quincy">H-Quincy</a> implements the paper <a href="http://www.sigops.org/sosp/sosp09/papers/isard-sosp09.pdf">Quincy: Fair Scheduling for Distributed Computing Clusters</a> on Hadoop, which improves the mapreduce scheduler by replacing the default queue-based one with a flow-based one. A min cost flow is calculated and updated to assign map tasks among the cluster, according to the size of the data split and the communication overhead in the cluster's hierarchy.</p>

<h2>Install</h2>

<p><code>bash
git clone https://github.com/puncsky/H-Quincy.git
</code></p>

<p>You can either build from source code or user the JAR directly.</p>

<ul>
<li><em>Build from Source Code</em>. Replace your <code>$HADOOP_HOME/src/mapred/org/apache/hadoop/mapred</code> with files in <code>src/</code>. Enter <code>$HADOOP_HOME</code> and build with <code>ant</code>.</li>
<li><em>utilize the JAR directly</em>. Replace your <code>$HADOOP_HOME/hadoop-core-{version}.jar</code> with <code>hadoop-core-1.0.4-SNAPSHOT.jar</code></li>
</ul>


<hr />

<p>The <a href="http://www.sigops.org/sosp/sosp09/papers/isard-sosp09.pdf">Quincy paper</a> is rather theoretical organized and involves a large number of mathematical details, which reasonably makes itself hard to understand. The following sections explains our implementation.</p>

<h2>1. Architecture</h2>

<p><em>Figure 1</em> displays an outline of our architecture. There exist three kinds of nodes and accordingly three levels of hierarchy in the cluster. Computing nodes are connected via a rack switch, and placed in the same rack. Rack switches are connected via a core switch. Core switches and rack switches do not undertake computing works but can still be presented in the Hadoop system as nodes.</p>

<p align="middle"><img src="https://raw.github.com/puncsky/H-Quincy/master/doc/architecture.png" alt="architecture"></p>




<p align="middle"><i>Figure 1: A sample architecture with simplified preferred lists.</i></p>




<!--more-->


<p>As we know, Hadoop takes a master/slave architecture, which includes a single master and multiple worker nodes. The master node in general consists of a JobTracker and a NameNode. The slave worker consist of a TaskTracker and a DataNode. Nevertheless, the master can also play the role of the slave at the same time. JobTracker assigns tasks to the TaskTrackers for execution, and then collects the result. NameNode manages the index for blocks of data storing in DataNodes among the Hadoop Distributed File System.</p>

<p>In our cluster, each computing node is both a TaskTracker and a DataNode, which means they are all slaves. And we select one of them as a master, which is both a JobTracker and a DataNode simultaneously. The master maintains the relationship with slaves through heartbeat, whose overhead is ignorable when compared to the data transfer or the execution of tasks.</p>

<p>There may be many jobs sharing the cluster resources at the same time. When a job comes into the JobTracker's job queue, the JobTracker resorts the queue by the priority and the start time of these jobs. A job is composed of the map tasks and the reduce tasks. When the job comes out of the queue and starts to run, the JobTracker will analyze the input data's distribution over those TaskTrackers and initialize a list of preferred tasks for each TaskTracker, rack switch, and core switch, as shown in <em>figure 1</em>. A task will occur on the preferred list of some node if its data splits are stored in that node <em>or</em> in any of its child nodes. Then the JobTracker's scheduler matches tasks to the TaskTrackers and launch them to run the tasks on their own newly-assigned lists. At the same time, the JobTracker keeps collecting status information from TaskTrackers until all the tasks finish. If failure happens in the TaskTracker, the JobTracker will restart the task from that TaskTracker or enable a new TaskTracker to execute the task. The scheduler can kill a task on a TaskTracker with preemption if there is a more suitable arrangement.</p>

<h2>2. Hadoop Default Scheduler</h2>

<p>After the initialization of preferred lists, the JobTracker assign a series of actions, including the tasks waiting for execution, into the response to heartbeat from the matched TaskTracker. The matched TaskTracker receives the heartbeat response and adds the actions to the task queue.</p>

<h3>2.1 Queue-based Greedy Scheduler without Fairness</h3>

<p>In the task assignment, the JobTracker's task scheduler first calculates the max workload for every job, and leaves certain padding slots for speculative tasks. The speculative task backs up some running task, in case that the running task is too slow and impede the job's progress. Map tasks are assigned before reduce tasks. Data-local and rack-local tasks are assigned before non-local tasks.</p>

<p>The default setup for Hadoop is a queue-based greedy scheduler. The preferred task lists of every node can be deemed as queues. Each computing node has a queue of tasks which can be executed without pulling data from other places. Each computing node in a rack shares a rack queue in case that computing node can execute tasks with pulling data splits from other nodes in the same rack, when the local list has already been finished. Since racks are connected with a core switch, racks also share a global queue. For every assignment, node will compute the previously failed tasks first, then the non-running local and non-local tasks, and finally the speculative tasks.</p>

<h3>2.2 Queue-based Greedy and Fair Scheduler</h3>

<p>When only one job is running on the cluster, that job will use the entire cluster. What will happen if multiple jobs are submitted? The default's sorting by priority and start time only ensures that more significant and earlier submitted jobs are dequeued first. However, the Hadoop fair scheduler arranges jobs into pools. Computing resources are divided fairly between these pools. A pool can be occupied by only one user (by default) or shared by a user group. Within the pool, jobs can be scheduled evenly or first-in-first-serve.</p>

<p>Assume we have $M$ computers and $K$ running jobs, and job $j$ has $N_{j}$ tasks in total. Each job $j$ will be allocated $A_{j} = min(\lfloor M/K\rfloor, N_{j})$ task slots. If there are still resting slots, divide them evenly. After finishing the running tasks, a TaskTracker will follow the new allocation $A_{j}$. However, if the process is not preemptive, the running tasks will definitely not be affected when new jobs are submitted irrespective of the changed allocation $A_{j}$. If the fairness is ensured with preemption, the running tasks will be killed while a new quota $A_{j}$ shows up.</p>

<p>Hadoop default scheduler sets up a wait time to enable a portion of the cluster wait to get better locality, preventing a job's tasks from becoming sticky to a TaskTracker.</p>

<h2>3. Quincy Scheduler</h2>

<p><a href="Quincy:%20Fair%20Scheduling%20for%20Distributed%20Computing%20Clusters">Quincy</a> introduces a new framework transforming the scheduling into a global min cost max flow problem. Running a particular task on some machine incurs a data calculation cost and, potentially, a data transfer cost. Killing a running task also incurs a wasted time cost. If different kinds of costs can be expressed in the same unit, then we can investigate an algorithm to minimize the total cost of the scheduling.</p>

<h3>3.1 Min Cost Max Flow</h3>

<p>In a flow network, a directed graph $G=(V, E)$ has $source \in V$ and $sink \in V$. For each edge $(u,v)\in E$, there are $capacity(u,v)\in\mathbb{N}$, $flow(u,v)\in\mathbb{N}$ and $cost(u,v)\in\mathbb{R}$.</p>

<p>The problem is to calculate the min cost flow</p>

<p>$min(\sum_{E}flow\cdot cost)$</p>

<p><a href="http://web.mit.edu/~ecprice/acm/acm08/MinCostMaxFlow.java">Edmonds-Karp algorithm</a> is used to calculate the min cost flow with $O(V\cdot E ^ 2)$ in our implementation.</p>

<h3>3.2 Initialization of the Matrix</h3>

<p><em>Figure 2</em> shows the graph along with the same topology in <em>Figure 1</em>. Since supplies are from a variety of sources -- task nodes and unscheduled nodes, the graph is a multi-source single-sink flow. Our implementation adds a virtual source to transform the flow into a single-source one.</p>

<p align="middle"><img src="https://raw.github.com/puncsky/H-Quincy/master/doc/mincostflow.png" alt="flow graph"></p>




<p align="middle"><i>Figure 2: Min-Cost Max Flow Graph</i></p>


<h4>3.2.1 Capacity</h4>

<p>Each task node has a supply of 1, so $capacity(source, task) = 1$. The <em>unscheduled</em> is used to control the fairness. Tasks flowing to the unscheduled will not be assigned to computing nodes at this time. Each job must have and only has one unscheduled node with $$capacity(source, unscheduled) = F_j - N_j$$ where $F_j$ is the max number of running tasks job j may has. $N_j$ is the number of TaskTrackers assigned to this job.</p>

<p>From each task node, there are edges to the core switch, preferred rack, and preferred computing nodes. By default, every split of data has three replicas, so the number of preferred computing nodes is usually 3. So we can yield $$capacity(task, core switch) = 1$$ $$capacity(task, preferredRackSwitch) = 1$$  $$capacity(task, preferredComputingNode) = 1$$</p>

<p>From the unscheduled, there is only one edge to sink with $capacity(unscheduled, sink) = F_j - E_j$, where $E_j$ is the min number of running tasks job j may have.</p>

<p>From the core switch, there are edges to every rack with capacities of $$capacity(coreSwitch, rackSwitch) = numberOfTaskTrackersInThatRack$$</p>

<p>From each rack switch, there are edges to every computing node with capacity of $capacity(rackSwitch, computingNode) = 1$.</p>

<p>From each computing node, there is only one edge with $capacity(computing node, sink) = numberOfTaskSlots$. The number of task slots is 2 by Hadoop's default for map tasks. The value is 1 for reduce tasks.</p>

<h4>3.2.2 Cost</h4>

<p>The cost of scheduling a task $t_n ^ j$ job $j$ with $n$ tasks onto a computing node is $\alpha_n ^ j = \psi  R ^ X(t_n ^ j) + \xi  X ^ X(t_n ^ j)$, where $\psi$ is the cost to transfer one GB across a rack switch, $\xi$ is the cost to transfer one GB across the core switch. $(R ^ X(t_n ^ j), X ^ X(t_n ^ j))$ is the upper bounds of the transferred data size across a rack switch and across a core switch.</p>

<p>The cost of scheduling a task onto a preferred rack is $\rho ^ j_{n,l} = \psi  R ^ R_l(t_n ^ j) + \xi  X ^ R_l(t_n ^ j)$.</p>

<p>The cost of scheduling a task onto a preferred computer is $\gamma ^ j_{n,m} = \psi  R ^ C_m(t_n ^ j) + \xi  X ^ C_m(t_n ^ j)$. However, if the computer is now executing the same task, the cost should be  $\gamma ^ j_{n,m} = \psi  R ^ C_m(t_n ^ j) + \xi  X ^ C_m(t_n ^ j) - \theta ^ j_n$, where $\theta$ is the number of seconds for which the task has been running.</p>

<p>The cost of scheduling a task onto the unscheduled is $\upsilon ^ j_n = \omega \nu ^ j_n$, where $\omega$ is a wait-time factor and $\nu ^ j_n$ is the total number of seconds that task $n$ in job $j$ has spent unscheduled.</p>

<p>In our current version for testing, the wait-time factor $\omega=0.5$, $\psi = 1$ per GB, $\xi = 2$ per GB. $\psi$ and $\xi$ can be set larger to achieve a better locality.</p>

<h3>3.3 Assignment and Update</h3>

<p>After initialization, the min cost flow matrix will be recalculated every time before a new task is assigned to a TaskTracker. When the job is running on the cluster, the capacity matrix and cost matrix will be updated if a task is finished. An edge from the finished task to the sink will be set with a capacity 1 and cost $-1000-\nu ^ j_n$.</p>

<h3>3.4 Preemption and Fairness</h3>

<p>There exist four versions of quincy.</p>

<ul>
<li>Quincy without Preemption and without Fairness(Q).</li>
<li>Quincy with Preemption and without Fairness(QP).</li>
<li>Quincy without Preemption and with Fairness(QF).</li>
<li>Quincy with Preemption and with Fairness(QPF).</li>
</ul>


<p>Limited to the time, our current implementation does not include preemption and fairness. Preemption is easy to achieve but there are more classes and source codes to modify for fairness control.</p>
]]></content>
  </entry>
  
</feed>
