<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: big_data | Puncsky CS Notebook]]></title>
  <link href="http://www.puncsky.com/blog/categories/big-data/atom.xml" rel="self"/>
  <link href="http://www.puncsky.com/"/>
  <updated>2016-02-13T02:42:15-08:00</updated>
  <id>http://www.puncsky.com/</id>
  <author>
    <name><![CDATA[Tian]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Crack the System Design Interview]]></title>
    <link href="http://www.puncsky.com/blog/2015/12/19/crack-the-system-design-interview/"/>
    <updated>2015-12-19T16:54:00-08:00</updated>
    <id>http://www.puncsky.com/blog/2015/12/19/crack-the-system-design-interview</id>
    <content type="html"><![CDATA[<p>How important is the system design interview? Not important for a new grad interviewing for an entry level position, but it will definitely be a highlight, if the new grad plays it well. Wait! Not all of us programmers are junior. And there are so many tutorials, frameworks, infrastructures to help you build your application. Building applications is not a hard thing, but having the vision of the overall architecture really makes a difference. So we have to crack the system interview sooner or later in our career as a software engineer or engineering manager.</p>

<p>Interviewers are looking for future teammates that they like to work with. The future teammates would better, at least, be capable of solving problems independently. There are so many choices for the solution, but not all of them are the best given some context. So the interviewee has to specify different choices and their tradeoffs. To summarize it, <strong>System design interview is a happy show-off of your knowledge on architectures, technologies, and tradeoffs. In an ideal case, keep talking what the interviewer expect throughout the interview, instead of letting them ask.</strong></p>

<p>Keep talking for 45 min could be easy, as long as you are armed with the following <strong>four steps and three common topics</strong>. Take <strong>"design Pinterest"</strong> for example.</p>

<h2>1. Four steps to Crack the System Design Interview</h2>

<p>Breaking up a complex task into small blocks helps you handle them with better paces and in actionable ways.</p>

<h3>1.1 Step 1. Clarify Requirements and Specs</h3>

<p>First things first, the ultimate goals should always be clear.</p>

<p>Pinterest is a highly scalable photo-sharing service:</p>

<ul>
<li>features: user profile, upload photos, news feed, etc.</li>
<li>scaling out: horizontal scalability and micro services.</li>
</ul>


<h3>1.2 Step 2. Sketch High Level Design</h3>

<p><strong>Do not dive into details before drawing the big picture.</strong> Otherwise, digging into an unwanted direction would prevent you from giving a roughly correct solution in the first place. You will regret if you do not have time to finish the task.</p>

<p>OK, let us sketch the following picture and do not explain too much about the detailed implementation of those components.</p>

<p>So far so good! To some extent, you have solved the problem. Surely this multi-tier architecture works, no matter how badly you would specify later!</p>

<h3>1.3 Step 3. Discuss individual components and how they interact in details</h3>

<p>When we truly understand a system, we should be able to identify what is what and explain how they interact. Look at those components above in the picture and specify one by one. This could lead to more general discussions, like three common topics in the next section, and to more specific domains, like how to design the photo storage data layout...</p>

<h4>1.3.1 Load Balancer</h4>

<p>Generally speaking, load balancers fall into three categories:</p>

<ul>
<li>DNS Round Robin (rarely used): clients get a randomly-ordered list of IP addresses.

<ul>
<li>pros: easy to implement and free</li>
<li>cons: hard to control and not responsive, since DNS cache needs time to expire</li>
</ul>
</li>
<li>L3/L4 Load Balancer: traffic is routed by IP address and ports.L3 is network layer (IP). L4 is session layer (TCP).

<ul>
<li>pros: better granularity, simple, responsive</li>
</ul>
</li>
<li>L7 Load Balancer: traffic is routed by what is inside the HTTP protocol. L7 is application layer (HTTP).</li>
</ul>


<p>It is good enough to talk to this level in the topic, but in case the interviewer wants more, we can suggest exact algorithms like round robin, weighted round robin, least loaded, least loaded with slow start, utilization limit, latency, cascade, etc.</p>

<h4>1.3.2 Reverse Proxy</h4>

<p>Reverse proxy, like varnish, centralizes internal services and provides unified interfaces to the external. For examole, www.example.com/index and www.example.com/sports feel like coming from the same domain, but they are from different micro services behind the reverse proxy. Reverse proxy could also help with caching and load balance.</p>

<h4>1.3.3 (Frontend) Web Tier</h4>

<p>This is where web pages are served, and usually combine with the service tier in the very early stage of a web service.</p>

<h5>Stateless</h5>

<p><strong>There are two major bottlenecks of the whole system -- requests per second and bandwidth.</strong> We could improve the situation by using more efficient tech stack, like frameworks with <a href="http://www.puncsky.com/blog/2015/01/13/understanding-reactor-pattern-for-highly-scalable-i-o-bound-web-server/">async and non-blocking reactor pattern</a>, and enhancing the hardware, like <strong>scaling up (aka vertical scaling) or scaling out (aka horizontal scaling)</strong>.</p>

<p>Internet companies prefer scaling out, since it is more cost-efficient with a huge number of commodity machines. This is also good for recruiting, because the target skillsets are equipped by most of the talents in the job market.</p>

<p><strong>Frontend web tier and service tier must be stateless, so it would be extremely convenient to add or remove hosts, and then horizontal scalability is achieved.</strong> There could be a standalone service keeping the configuration states for other services.</p>

<h5>Web Application and API</h5>

<p><strong>MVC(MVT) or MVVC</strong> pattern is the dominant pattern in this layer. In the age of mobile computing, view can be as simple as serving the minimal package of data to the mobile devices, which is called <strong>web API</strong>. People believe the API could be shared by clients and browsers. And that is why <strong>single page web application</strong> becoming popular, especially with the assistance of frontend frameworks like react.js, angular.js, backbone.js, etc.</p>

<h4>1.3.4 App Service Tier</h4>

<p><strong>The single responsibility principle</strong> advocates small and autonomous services that work together, so that each service could do one thing well and do not block others. And small teams with small services can plan much more aggressively, for the sake of hyper-growth.</p>

<h5>Service Discovery</h5>

<p>How do those services find each other? <a href="http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper">Zookeeper</a> is a popular and centralized choice. Instances with name, address, port, etc. are registered into the path in ZooKeeper for each service. If one service does not know where to find another service, query Zookeeper for the location and remember it until unavailable.</p>

<p>Zookeeper is CP system in terms of CAP theorem, which means it is consistent in case of failures, but the leader will be not available for registering new services.</p>

<p>In contrast to Zookeeper, Uber is doing interesting work in a decentralized way, named <a href="https://github.com/uber/hyperbahn">hyperbahn</a>, based on <a href="https://github.com/uber/ringpop-node">Ringpop consisten hash ring</a>. Read Amazon's Dynamo for understanding AP and eventual consistency.</p>

<h5>Micro Services</h5>

<p>For designing Pinterest, those specific services could be user profile, follower, feed, search, spam, etc. Any of those topics could lead to in-depth discussions. Useful links are listed in the last section, future studies, to help you deal with them.</p>

<p>However, for a generalized interview question like "design Pinterest", it is good enough for just having those services drawn as boxes. If we want to show more passions, trying to write down some sample endpoints / APIs for those services would be great.</p>

<h4>1.3.5  Data Tier</h4>

<p>Although a relational database can do almost all the storage work, please remember <strong>do not save blob, like photo, into a relational database, and choose the right database for the right service.</strong> As displayed in the architecture graph, follower service is demanding on read performance, so use a KV cache. Feeds are generated as time passing by, HBase's timestamp index fits the user case very well. Users have relationships with other users or objects, so a relational database is our choice by default in a user profile service.</p>

<p>Data and storage is a rather broad and general topic, and we will discuss it later in Section 2.</p>

<h3>1.4 (Optional) Step 4. Back-of-the-envelop Calculation</h3>

<p>The final step about estimating how many machines are required is absolutely optional, because time is almost up after all the discussions above and three common topics below. In case you meet this, you would better prepare it as well. It is a little tricky... you need to assume variables and give a function first, and then guess the exact values for those variables, and finally calculate the result.</p>

<p>The cost is a function of CPU, RAM, storage, bandwidth, number and size of the images uploaded each day.</p>

<ul>
<li><em>N</em> users 10<sup>10</sup></li>
<li><em>i</em> images / (user * day) 10</li>
<li><em>s</em> size in bytes / image 10<sup>6</sup></li>
<li>viewed <em>v</em> times / image 100</li>
<li><em>d</em> days</li>
<li><em>h</em> requests / sec 10<sup>4</sup>   (bottleneck)</li>
<li><em>b</em> bandwidth (bottleneck)</li>
<li>Server cost: $1000 / server</li>
<li>Storage cost: $0.1 / GB</li>
<li>Storage = Nisd</li>
</ul>


<p><em>Number of required servers = max(Niv/h, Nivs/b)</em></p>

<h2>2 Three common Topics</h2>

<p>In Section 1.3 above, there are three common topics that could be applied to almost every system design question. They are extracted and summarized in this section.</p>

<h3>2.1 Communication</h3>

<p>How do different components interact with each other? -- communication protocols.</p>

<p>General comparison of those protocols.</p>

<ul>
<li>UDP and TCP are both transport layer protocols. TCP is reliable and connection-based. UDP is connectionless and unreliable.</li>
<li>HTTP is in application layer. It is normally TCP based since HTTP assumes a reliable transport.</li>
<li>RPC, an application layer protocol, is an inter-process communication that allows a computer program to cause a subroutine or procedure to execute in another address space (commonly on another computer on a shared network), without the programmer explicitly coding the details for this remote interaction. That is, the programmer writes essentially the same code whether the subroutine is local to the executing program, or remote. When the software in question uses object-oriented principles, RPC is called remote invocation or remote method invocation (RMI).</li>
</ul>


<p>Here is a graph showing how RPC works.</p>

<p>Stub procedure: a local procedure to marshal the procedure identifier and the arguments into a request message, which it sends via its communication module to the server. When the reply message arrives, it unmarshals the results.</p>

<p>HTTP APIs are usually following RESTful style.</p>

<ul>
<li>REST(Representational state transfer of resources)

<ul>
<li>Best practice of HTTP API to interact with resources.</li>
<li>URL only decides the location. Headers (Accept and Content-Type, etc.) decide the representation. HTTP methods(GET/POST/PUT/DELETE) decide the state transfer.</li>
<li>minimize the coupling between client and server (a huge number of HTTP infras on various clients, data-marshalling).</li>
<li>Stateless and scaling out.</li>
<li>service partitioning feasible.</li>
<li>Public API.</li>
</ul>
</li>
</ul>


<p>You do not have to implement your own RPC protocols, since there are off-the-shelf frameworks.</p>

<ul>
<li>Google Protobuf: open source version does not have RPC implementations but only API. Smaller serialized data and slightly faster. Better documentations and cleaner APIs.</li>
<li>Facebook Thrift: support more languages, richer data structures(support richer data structures: List/Set/Map, etc，Protobuf不支持。Usercase: Hbase/Cassandra/Hypertable/Scrib/..  Incomplete documentation and hard to find good examples.</li>
<li>Apache Avro: Avro is heavily used in the hadoop ecosystem. It is based on dynamic schemas in Json. It features dynamic typing, untagged data, and no manually-assigned field IDs.

<h3>2.2 Storage</h3></li>
</ul>


<h3>2.3 CAP</h3>

<p>Consistency: all nodes see the same data at the same time
Availability: a guarantee that every request receives a response about whether it succeeded or failed
Partition tolerance: system continues to operate despite arbitrary message loss or failure of part of the system</p>

<p>三者只能取二，分布式系统通常AP, CP取一</p>

<p>CAP 原则是第一原则，系统设计之初就考虑侧重 CP 还是 AP</p>

<p>rDBMS 取 CP
NoSQL 取 AP</p>

<p>Cold standby, Warm standby, Hot standby, Active-active</p>

<h2>3 Future Studies</h2>

<p>Just as everything else, preparation and practice always makes perfect.</p>

<h2>- <a href="https://github.com/checkcheckzz/system-design-interview">System design interview for IT companies</a></h2>

<p>EOF</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[面向痛苦编程 (译文)]]></title>
    <link href="http://www.puncsky.com/blog/2015/05/06/suffering-oriented-programming/"/>
    <updated>2015-05-06T18:04:00-07:00</updated>
    <id>http://www.puncsky.com/blog/2015/05/06/suffering-oriented-programming</id>
    <content type="html"><![CDATA[<p>Nathan Marz 是 Twitter 的分布式流处理框架 Storm 的主要作者，他的新书 <em>Big Data: Principles and best practices of scalable realtime data systems</em> 今天正式<a href="http://www.amazon.com/dp/1617290343">发售</a>了，我半个月前就下了订单，满心期盼。先前在浏览他的<a href="http://nathanmarz.com/">博客</a>时看到一篇有趣的文章，发表于2012年，现中文翻译于此，和大家分享。</p>

<hr />

<p>面向痛苦编程</p>

<p>作者：Nathan Marz</p>

<p>译者：潘天 (puncsky)</p>

<p>前些天，有人问了我个有趣的问题：“在创业公司工作的时候，还能够有勇气去写 <a href="http://storm.apache.org/">Storm</a>，你是怎样做到的？” 我知道，在外界看来，创业公司做如此大规模的项目，是相当冒险的行为。而在我自己看来，其实，写 Storm 一点儿风险都没有：它很有挑战性，但并没有风险。</p>

<p>我遵从了一种能够极大地规避风险的开发方式，来应对像 Storm 这种大型项目，并将其命名为 “面向痛苦编程” 。简单来说，面向痛苦编程是这样的：不去做那些不重要的项目，除非，缺少它真的让你感到痛苦难耐。这一准则放之四海而皆准，大到架构上的决策，小到每天写的代码。面向痛苦编程保证你总是在做重要的事情，在尝试大的投入前先从小处精通问题的本质，因此而能够极大地规避风险。</p>

<p>面向痛苦编程有三句箴言：“先做成，再做美，后做快。” (First make it possible. Then make it beautiful. Then make it fast.)</p>

<h2>先做成</h2>

<p>当你进入某个你不熟悉的问题领域 (problem domain) 时，不要立马写一个 “通用的” 或者 “可拓展” 的解。你都不太了解，如何预见未来会有哪些需求？你会把不必通用的变通用，浪费了时间，增加了复杂性。</p>

<p>更好的做法是仅仅 “三下五除二干出来再说” (hack things out) ，直面解决手头的问题。这样，你就能够做成你需要做成的事儿，避免浪费时间。</p>

<p>Storm 的 “先做成” 阶段是一年的快速开发，用队列和工作节点做一个流试处理处理系统。我们学到了如何用 “ack” 协议保证数据处理，我们学到了如何用集群的队列和工作节点规模化实时计算，我们学到了有时候要用不同的方式分割 (partition) 消息流：有时候随机地、有时候 hash/mod 地，确保同样的东西总是流到同样的工作节点上。</p>

<p>当时我们还甚至不知道我们处在 “先做成” 的阶段，只是专注于做产品。尽管队列和工作节点们形成的系统在后来很快让我们痛得不行，但是当时，规模化这个系统很乏味，我们也并不需要容错机制。很明显，队列和工作节点形成的范式并不是一层正确的抽象，因为我们大多数的代码与路由 (routing messages) 和序列化有关，并非我们真正关心的业务逻辑。</p>

<p>同时，开发产品让我们发现了 “实时计算” 的新用例。我们写了一个功能，计算 Twitter 上某一个 URL 的 受众量 (Reach)，所谓受众量，就是能够接触到 Twitter 上的某个 URL 的去除重复后的人数。这一计算很困难，一次计算就需要上百次的数据库访问、和上千万的去重操作。要处理这些绝对路径的 URL，我们最初的单机实现要跑一分多钟，显然，我们需要某种分布式系统来并行处理，加速计算。</p>

<p>启发 Storm 的一个关键就在于，我们意识到了 “受众量问题” 和 “流处理问题” 可以被归化成简单的抽象。</p>

<h2>再做美</h2>

<p>当你三下五除二干出来再说 (hacking things out) 的时候，这一问题领域的 “地图” 也逐渐明晰起来。随着时间的推移，你逐渐习得这一领域越来越多的用例，深入了解这些系统的微妙与复杂之处。这些深入的了解能够启发 “美丽” 的技术，来替代现有的系统，减轻你的痛苦，让过去的不可能做到的新系统、新功能变成可能。</p>

<p>做“美”的关键在于，搞清楚能够解决已知具体问题的最简抽象集。不要预测那些实际上你并未遇到的具体用例，否则结果就会做得太过了 (overengineering)。经验法则是，要想投入更多，对问题领域的理解需要更深刻，用例需要更多样。否则，就有发生<a href="http://en.wikipedia.org/wiki/Second-system_effect">第二系统效应</a>的风险。</p>

<p>在“做美”阶段，你动用设计与抽象的技能，把问题区间提炼成简单的、可组合在一起的抽象们。提炼美好的抽象就似统计学所谓的回归一般：图上一堆点 (用例) ，找条最简单的曲线 (抽象) 契合这些点。</p>

<p>用例越多，找契合点的曲线也就越方便。如果点不够，得到的曲线要么做得太过，要么做得不够好，最终过度工程或者浪费时间。</p>

<p>做美有很大一块是去了解问题区间的性能与资源特征 (understanding the performance and resource characteristics of the problem space)，设计优美的解要利用那些属于前一阶段“做成”时学到的微妙与复杂。</p>

<p>就 Storm 而言，我把实时处理提炼成了一堆小的抽象：流 (streams)，出水口 (spouts)，水栓 (bolts)，和拓扑结构 (topologies)。我发明了新的算法，消除中间消息的代理，同时又能够保证数据得到了处理，整个系统中最复杂最令人痛苦的部分因此而被除去。流处理和受众量，两个看上去迥异的问题，如此优雅地归化到了 Storm，这强烈地预示着，我做的东西，了不得。</p>

<p>我继而进一步地为 Storm 寻找更多的用例，验证我的设计。我和其他工程师深入讨论，学习他们处理实时问题的独到之处。我并没有只去问我认识的人，我还发了 Twitter 说我正在做一款新的实时系统，想要知道其他人的用例，随后有很多有趣的讨论，让我对问题领域有了更深的认识，并验证了我的设计思路。</p>

<h2>后做快</h2>

<p>一旦得到了优美的设计，就能安全地花时间做性能分析 (profiling) 和优化了。过早优化浪费时间，因为你仍然有可能重新做设计。</p>

<p>“做快”并不是指系统高层级的性能特征。这些特征本应该在“做成”阶段被理解到，在“做美”阶段被设计好。“做快”是指微观上的优化，让代码更紧凑，更有效率地利用资源。所以说，在“做美”阶段，你会关心诸如渐进复杂度的问题；在“做快”阶段，你会关心诸如复杂度中那些常量的问题。</p>

<h2>反复雕琢</h2>

<p>面向痛苦编程是持续的过程。构建美丽的系统赋予你新的能力，让你在问题区间里更新更深的领域有能力“做成”。学到的新的知识又反馈到原有的技术中，用以微调或者补充原有的抽象，这样就能够搞定更多的用例。</p>

<p>Storm 就像这样迭代了好多次。一开始使用 Storm 的时候，我们发现了，单一的组件需要有能力放出多个独立的流。我们发现了，Storm 批处理元组 (process batches of tuples) 作为具体的单元，需要增加一种特殊的“直接流 (direct stream) ”。最近，我又开发了“事务性拓扑 (transactional topologies)” ，在 Storm 至少处理一次的基础上，对几乎任意情况的实时处理需求，达成恰好通知一次的语义。</p>

<p>当然了，在自己不了解的领域三下五除二干出来再说，然后持续性地迭代，会导致一些渣代码。而面向痛苦编程最重要的特性便是大无畏地专注于重构。这正是规避“<a href="http://en.wikipedia.org/wiki/Accidental_complexity">偶发复杂度 (accidental complexity)</a>” (译者注：将概念上的构思施行于电脑上所遭遇到的困难。) 破坏项目的核心所在。</p>

<h2>结论</h2>

<p>用例在面向痛苦编程中至关重要，用黄金来形容它们的价值也不为过。而习得用例的唯一方法，就是写代码积累经验。</p>

<p>绝大多数的程序员都会经历这样的进化过程。一开始挣扎地让东西跑起来，代码乱得毫无章法，渣代码和拷贝粘贴的代码一塌糊涂。后来，你会发现结构化编程的好处，尽可能多地共享代码逻辑。然后，你学到了如何抽象，用封装的思想看系统。再后来，你着迷于通用化你的代码，让一切的一切可以拓展到未来的程序中。</p>

<p>面向痛苦编程拒绝相信人们能够有效地预测未知的需求，它承认，在不了解问题领域的情况下通用化代码，终将导致复杂与浪费。设计一定要，也总是要，驱动于活生生的用例。</p>

<p>EOF</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[H-Quincy: Fair Scheduling for Hadoop Clusters]]></title>
    <link href="http://www.puncsky.com/blog/2013/09/14/h-quincy/"/>
    <updated>2013-09-14T16:49:00-07:00</updated>
    <id>http://www.puncsky.com/blog/2013/09/14/h-quincy</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/puncsky/H-Quincy">H-Quincy</a> implements the paper <a href="http://www.sigops.org/sosp/sosp09/papers/isard-sosp09.pdf">Quincy: Fair Scheduling for Distributed Computing Clusters</a> on Hadoop, which improves the mapreduce scheduler by replacing the default queue-based one with a flow-based one. A min cost flow is calculated and updated to assign map tasks among the cluster, according to the size of the data split and the communication overhead in the cluster's hierarchy.</p>

<h2>Install</h2>

<p><code>bash
git clone https://github.com/puncsky/H-Quincy.git
</code></p>

<p>You can either build from source code or user the JAR directly.</p>

<ul>
<li><em>Build from Source Code</em>. Replace your <code>$HADOOP_HOME/src/mapred/org/apache/hadoop/mapred</code> with files in <code>src/</code>. Enter <code>$HADOOP_HOME</code> and build with <code>ant</code>.</li>
<li><em>utilize the JAR directly</em>. Replace your <code>$HADOOP_HOME/hadoop-core-{version}.jar</code> with <code>hadoop-core-1.0.4-SNAPSHOT.jar</code></li>
</ul>


<hr />

<p>The <a href="http://www.sigops.org/sosp/sosp09/papers/isard-sosp09.pdf">Quincy paper</a> is rather theoretical organized and involves a large number of mathematical details, which reasonably makes itself hard to understand. The following sections explains our implementation.</p>

<h2>1. Architecture</h2>

<p><em>Figure 1</em> displays an outline of our architecture. There exist three kinds of nodes and accordingly three levels of hierarchy in the cluster. Computing nodes are connected via a rack switch, and placed in the same rack. Rack switches are connected via a core switch. Core switches and rack switches do not undertake computing works but can still be presented in the Hadoop system as nodes.</p>

<p align="middle"><img src="https://raw.github.com/puncsky/H-Quincy/master/doc/architecture.png" alt="architecture"></p>




<p align="middle"><i>Figure 1: A sample architecture with simplified preferred lists.</i></p>




<!--more-->


<p>As we know, Hadoop takes a master/slave architecture, which includes a single master and multiple worker nodes. The master node in general consists of a JobTracker and a NameNode. The slave worker consist of a TaskTracker and a DataNode. Nevertheless, the master can also play the role of the slave at the same time. JobTracker assigns tasks to the TaskTrackers for execution, and then collects the result. NameNode manages the index for blocks of data storing in DataNodes among the Hadoop Distributed File System.</p>

<p>In our cluster, each computing node is both a TaskTracker and a DataNode, which means they are all slaves. And we select one of them as a master, which is both a JobTracker and a DataNode simultaneously. The master maintains the relationship with slaves through heartbeat, whose overhead is ignorable when compared to the data transfer or the execution of tasks.</p>

<p>There may be many jobs sharing the cluster resources at the same time. When a job comes into the JobTracker's job queue, the JobTracker resorts the queue by the priority and the start time of these jobs. A job is composed of the map tasks and the reduce tasks. When the job comes out of the queue and starts to run, the JobTracker will analyze the input data's distribution over those TaskTrackers and initialize a list of preferred tasks for each TaskTracker, rack switch, and core switch, as shown in <em>figure 1</em>. A task will occur on the preferred list of some node if its data splits are stored in that node <em>or</em> in any of its child nodes. Then the JobTracker's scheduler matches tasks to the TaskTrackers and launch them to run the tasks on their own newly-assigned lists. At the same time, the JobTracker keeps collecting status information from TaskTrackers until all the tasks finish. If failure happens in the TaskTracker, the JobTracker will restart the task from that TaskTracker or enable a new TaskTracker to execute the task. The scheduler can kill a task on a TaskTracker with preemption if there is a more suitable arrangement.</p>

<h2>2. Hadoop Default Scheduler</h2>

<p>After the initialization of preferred lists, the JobTracker assign a series of actions, including the tasks waiting for execution, into the response to heartbeat from the matched TaskTracker. The matched TaskTracker receives the heartbeat response and adds the actions to the task queue.</p>

<h3>2.1 Queue-based Greedy Scheduler without Fairness</h3>

<p>In the task assignment, the JobTracker's task scheduler first calculates the max workload for every job, and leaves certain padding slots for speculative tasks. The speculative task backs up some running task, in case that the running task is too slow and impede the job's progress. Map tasks are assigned before reduce tasks. Data-local and rack-local tasks are assigned before non-local tasks.</p>

<p>The default setup for Hadoop is a queue-based greedy scheduler. The preferred task lists of every node can be deemed as queues. Each computing node has a queue of tasks which can be executed without pulling data from other places. Each computing node in a rack shares a rack queue in case that computing node can execute tasks with pulling data splits from other nodes in the same rack, when the local list has already been finished. Since racks are connected with a core switch, racks also share a global queue. For every assignment, node will compute the previously failed tasks first, then the non-running local and non-local tasks, and finally the speculative tasks.</p>

<h3>2.2 Queue-based Greedy and Fair Scheduler</h3>

<p>When only one job is running on the cluster, that job will use the entire cluster. What will happen if multiple jobs are submitted? The default's sorting by priority and start time only ensures that more significant and earlier submitted jobs are dequeued first. However, the Hadoop fair scheduler arranges jobs into pools. Computing resources are divided fairly between these pools. A pool can be occupied by only one user (by default) or shared by a user group. Within the pool, jobs can be scheduled evenly or first-in-first-serve.</p>

<p>Assume we have $M$ computers and $K$ running jobs, and job $j$ has $N_{j}$ tasks in total. Each job $j$ will be allocated $A_{j} = min(\lfloor M/K\rfloor, N_{j})$ task slots. If there are still resting slots, divide them evenly. After finishing the running tasks, a TaskTracker will follow the new allocation $A_{j}$. However, if the process is not preemptive, the running tasks will definitely not be affected when new jobs are submitted irrespective of the changed allocation $A_{j}$. If the fairness is ensured with preemption, the running tasks will be killed while a new quota $A_{j}$ shows up.</p>

<p>Hadoop default scheduler sets up a wait time to enable a portion of the cluster wait to get better locality, preventing a job's tasks from becoming sticky to a TaskTracker.</p>

<h2>3. Quincy Scheduler</h2>

<p><a href="Quincy:%20Fair%20Scheduling%20for%20Distributed%20Computing%20Clusters">Quincy</a> introduces a new framework transforming the scheduling into a global min cost max flow problem. Running a particular task on some machine incurs a data calculation cost and, potentially, a data transfer cost. Killing a running task also incurs a wasted time cost. If different kinds of costs can be expressed in the same unit, then we can investigate an algorithm to minimize the total cost of the scheduling.</p>

<h3>3.1 Min Cost Max Flow</h3>

<p>In a flow network, a directed graph $G=(V, E)$ has $source \in V$ and $sink \in V$. For each edge $(u,v)\in E$, there are $capacity(u,v)\in\mathbb{N}$, $flow(u,v)\in\mathbb{N}$ and $cost(u,v)\in\mathbb{R}$.</p>

<p>The problem is to calculate the min cost flow</p>

<p>$min(\sum_{E}flow\cdot cost)$</p>

<p><a href="http://web.mit.edu/~ecprice/acm/acm08/MinCostMaxFlow.java">Edmonds-Karp algorithm</a> is used to calculate the min cost flow with $O(V\cdot E ^ 2)$ in our implementation.</p>

<h3>3.2 Initialization of the Matrix</h3>

<p><em>Figure 2</em> shows the graph along with the same topology in <em>Figure 1</em>. Since supplies are from a variety of sources -- task nodes and unscheduled nodes, the graph is a multi-source single-sink flow. Our implementation adds a virtual source to transform the flow into a single-source one.</p>

<p align="middle"><img src="https://raw.github.com/puncsky/H-Quincy/master/doc/mincostflow.png" alt="flow graph"></p>




<p align="middle"><i>Figure 2: Min-Cost Max Flow Graph</i></p>


<h4>3.2.1 Capacity</h4>

<p>Each task node has a supply of 1, so $capacity(source, task) = 1$. The <em>unscheduled</em> is used to control the fairness. Tasks flowing to the unscheduled will not be assigned to computing nodes at this time. Each job must have and only has one unscheduled node with $$capacity(source, unscheduled) = F_j - N_j$$ where $F_j$ is the max number of running tasks job j may has. $N_j$ is the number of TaskTrackers assigned to this job.</p>

<p>From each task node, there are edges to the core switch, preferred rack, and preferred computing nodes. By default, every split of data has three replicas, so the number of preferred computing nodes is usually 3. So we can yield $$capacity(task, core switch) = 1$$ $$capacity(task, preferredRackSwitch) = 1$$  $$capacity(task, preferredComputingNode) = 1$$</p>

<p>From the unscheduled, there is only one edge to sink with $capacity(unscheduled, sink) = F_j - E_j$, where $E_j$ is the min number of running tasks job j may have.</p>

<p>From the core switch, there are edges to every rack with capacities of $$capacity(coreSwitch, rackSwitch) = numberOfTaskTrackersInThatRack$$</p>

<p>From each rack switch, there are edges to every computing node with capacity of $capacity(rackSwitch, computingNode) = 1$.</p>

<p>From each computing node, there is only one edge with $capacity(computing node, sink) = numberOfTaskSlots$. The number of task slots is 2 by Hadoop's default for map tasks. The value is 1 for reduce tasks.</p>

<h4>3.2.2 Cost</h4>

<p>The cost of scheduling a task $t_n ^ j$ job $j$ with $n$ tasks onto a computing node is $\alpha_n ^ j = \psi  R ^ X(t_n ^ j) + \xi  X ^ X(t_n ^ j)$, where $\psi$ is the cost to transfer one GB across a rack switch, $\xi$ is the cost to transfer one GB across the core switch. $(R ^ X(t_n ^ j), X ^ X(t_n ^ j))$ is the upper bounds of the transferred data size across a rack switch and across a core switch.</p>

<p>The cost of scheduling a task onto a preferred rack is $\rho ^ j_{n,l} = \psi  R ^ R_l(t_n ^ j) + \xi  X ^ R_l(t_n ^ j)$.</p>

<p>The cost of scheduling a task onto a preferred computer is $\gamma ^ j_{n,m} = \psi  R ^ C_m(t_n ^ j) + \xi  X ^ C_m(t_n ^ j)$. However, if the computer is now executing the same task, the cost should be  $\gamma ^ j_{n,m} = \psi  R ^ C_m(t_n ^ j) + \xi  X ^ C_m(t_n ^ j) - \theta ^ j_n$, where $\theta$ is the number of seconds for which the task has been running.</p>

<p>The cost of scheduling a task onto the unscheduled is $\upsilon ^ j_n = \omega \nu ^ j_n$, where $\omega$ is a wait-time factor and $\nu ^ j_n$ is the total number of seconds that task $n$ in job $j$ has spent unscheduled.</p>

<p>In our current version for testing, the wait-time factor $\omega=0.5$, $\psi = 1$ per GB, $\xi = 2$ per GB. $\psi$ and $\xi$ can be set larger to achieve a better locality.</p>

<h3>3.3 Assignment and Update</h3>

<p>After initialization, the min cost flow matrix will be recalculated every time before a new task is assigned to a TaskTracker. When the job is running on the cluster, the capacity matrix and cost matrix will be updated if a task is finished. An edge from the finished task to the sink will be set with a capacity 1 and cost $-1000-\nu ^ j_n$.</p>

<h3>3.4 Preemption and Fairness</h3>

<p>There exist four versions of quincy.</p>

<ul>
<li>Quincy without Preemption and without Fairness(Q).</li>
<li>Quincy with Preemption and without Fairness(QP).</li>
<li>Quincy without Preemption and with Fairness(QF).</li>
<li>Quincy with Preemption and with Fairness(QPF).</li>
</ul>


<p>Limited to the time, our current implementation does not include preemption and fairness. Preemption is easy to achieve but there are more classes and source codes to modify for fairness control.</p>
]]></content>
  </entry>
  
</feed>
