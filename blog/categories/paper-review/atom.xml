<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: paper_review | CS Notebook]]></title>
  <link href="http://puncsky.github.com/blog/categories/paper-review/atom.xml" rel="self"/>
  <link href="http://puncsky.github.com/"/>
  <updated>2013-01-28T14:23:59-05:00</updated>
  <id>http://puncsky.github.com/</id>
  <author>
    <name><![CDATA[Tian]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[HadoopDB (VLDB'09)]]></title>
    <link href="http://puncsky.github.com/blog/2013/01/27/hadoopdb-vldb-09/"/>
    <updated>2013-01-27T15:50:00-05:00</updated>
    <id>http://puncsky.github.com/blog/2013/01/27/hadoopdb-vldb-09</id>
    <content type="html"><![CDATA[<p><a href="http://db.cs.yale.edu/hadoopdb/hadoopdb.html">Yale DB</a></p>

<h3>1. Problem</h3>

<p>Large analytical data management with commodity clusters</p>

<h3>2. Challenges</h3>

<ul>
<li>Performance and efficiency

<ul>
<li>hadoop

<ul>
<li>is not for structured data analysis</li>
<li>scale well</li>
<li>open source, without cost</li>
</ul>
</li>
</ul>
</li>
<li>scalability, fault-tolerance, and flexibility

<ul>
<li>Previous parallel db fit into tens of nodes, not thousand of nodes.

<ul>
<li>not scale well for failures, heterogeneous machines, performance untested</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3>3. Solutions</h3>

<ul>
<li>MapReduce (Hadoop) + parallel db (or single-node DBs) = HadoopDB
  Translation layer: Hive
  Communication layer: Hadoop
  Database layer: PostgreSQL (or MySQL, ... JDBC)</li>
<li>Components

<ol>
<li>Database Connector

<ul>
<li>Interface among dbs on nodes</li>
<li>extends InputFormat class -> InputFormat Implementation lib</li>
<li>JDBC-complaint</li>
</ul>
</li>
<li>Catalog

<ul>
<li>metainformation as XML = connection para + metadata</li>
</ul>
</li>
<li>Data Loader

<ul>
<li>Global Hasher: MR, repartition raw data from HDFS to NO. of nodes</li>
<li>Local Hasher: copy from HDFS, repartition the partition into chunks</li>
<li>Better load balance than Hadoop</li>
</ul>
</li>
<li>SQL - MR - SQL (SMS) Planner

<ul>
<li>extend Hive for

<ul>
<li>open and low cost</li>
<li>each table stored separately in HDFS, low performance in multi-table trans.</li>
</ul>
</li>
<li>intercept normal Hive flow in

<ol>
<li>update MetaStore before query execution</li>
<li>between query plan generation and MR jobs
 retrieve fields, determine partition keys
 traverse DAG bottom-up
only support filter, select, aggregation</li>
</ol>
</li>
</ul>
</li>
</ol>
</li>
<li>Extend performance [23] with fault tolerance and heterogeneous node exp

<ul>
<li>TODO: modify the current task scheduler, connect not straggler node but the replicas.</li>
</ul>
</li>
<li>Exp on EC2

<ul>
<li>Hadoop</li>
<li>HadoopDB</li>
<li>Vertica</li>
<li>DBMS-X</li>
</ul>
</li>
</ul>


<h3>4. Conclusion</h3>

<ul>
<li>HadoopDB's performance &lt; parallel db for
  PostgreSQL (not column-store, not compression)
  Hadoop and Hive are young</li>
<li>performance, heterogeneous environ., fault tolerance, flexibility</li>
</ul>


<h3>Related Readings</h3>

<p>[23] A Comparison of Approaches to Large Scale Data Analysis
[6] Scope [11] Hive [24] C-store [4] What is the right way to measure scale?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CryptDB: Protecting Confidentiality with Encrypted Query Processing (SOSP '11)]]></title>
    <link href="http://puncsky.github.com/blog/2012/11/26/cryptdb/"/>
    <updated>2012-11-26T19:45:00-05:00</updated>
    <id>http://puncsky.github.com/blog/2012/11/26/cryptdb</id>
    <content type="html"><![CDATA[<p><a href="http://css.csail.mit.edu/cryptdb/">MIT CSAIL</a>, <a href="http://people.csail.mit.edu/nickolai/papers/raluca-cryptdb.pdf">papers</a>, <a href="http://www.sigops.org/sosp/sosp11/current/2011-Cascais/07-popa.pptx">slides</a></p>

<h3>1. Problem</h3>

<p>Provide confidentiality for applications using DBMSes to solve two threats:</p>

<ol>
<li><em>Passive DB server attacks</em>. The curious database administrator (DBA) learns private data.</li>
<li><em>Any attacks on all servers</em>. The adversary gains complete control over application and DBMS servers.</li>
</ol>


<!--more-->


<h3>2. Challenges</h3>

<ol>
<li>Tension between confidentiality and efficiency. (Threat 1 + 2)

<ul>
<li>too slow or not enough confidentiality</li>
<li>incapable of executing SQL queries without giving keys to DBMS servers</li>
</ul>
</li>
<li>Support user-shared data while minimizing data leakage when both application servers and DBMS servers are compromised.

<ul>
<li>“Different keys for different users” alone is not suitable for user-shared data.</li>
</ul>
</li>
</ol>


<h3>3. Solutions</h3>

<p>How to solve Challenge 1 (Threat 1+2) while still retaining data sharing (Challenge 2)?</p>

<p>Architecture Overview</p>

<p><img src="http://puncsky.github.com/images/cryptdb/arch.png" alt="architecture overview" /></p>

<ol>
<li>No plaintext in database at all. (eliminate Threat 1)</li>
<li>Fine-grind keys chained to user passwords. (eliminate Threat 2 and Challenge 2)</li>
</ol>


<h4>3.1 Solution to Threat 1</h4>

<p>How to deal with <em>Passive DB server attacks</em>? No plaintext in database at all.</p>

<h5>3.1.1 Execute SQL queries over encrypted data</h5>

<p>Six types of SQL-aware encryption</p>

<p>SQL-aware encryption strategy with symmetric-key encryption (for efficiency).</p>

<ol>
<li>Random (RND)

<ul>
<li>support indistinguishability under an adaptive chosen-plaintext attack (IND-CPA), same plain texts -> different cipher texts</li>
<li>AES / Blowfish in CBC mode, a random initialization vector (IV)</li>
</ul>
</li>
<li>Deterministic (DET)

<ul>
<li>support equality checks, same plain texts -> same cipher texts</li>
<li>different keys for different columns to prevent cross-column correlations</li>
<li>Pseudo-random permutation (PRP), Blowfish for 64-bit block, AES for 128-bit block</li>
<li>AES in CMC mode (= one round of CBC + another round of CBC in reverse order), zero IV. (to prevent leakage of prefix equality in CBC mode)</li>
</ul>
</li>
<li>Order-preserving (OPE)

<ul>
<li>support order relations. If x &lt; y, then OPEk(x) &lt; OPEk(y) for any secret key K.</li>
<li>Weaker. Thus, OPE-encrypted columns are revealed to the server only when the order query is needed.</li>
<li>Implementation and optimization of <a href="http://www.cc.gatech.edu/~aboldyre/papers/bclo.pdf">Order-preserving symmetric encryption</a>. Use AVL BST for batch encryption. 25 ms/encryption -> 7 ms/encryption</li>
</ul>
</li>
<li>Homomorphic encryption(HOM)

<ul>
<li>retains IND-CPA while allows computations to be conducted on ciphertext and obtain an encrypted result which is the ciphertext of the result of operations performed on the plaintext.</li>
<li>UDF calling Paillier crytosystem</li>
<li>e.g. SUM: HOMk(x)*HOMk(y)=HOMk(x+y)</li>
</ul>
</li>
<li>Join (JOIN and OPE-JOIN)

<ul>
<li>support joins between two columns (because of DET)</li>
<li>TODO</li>
</ul>
</li>
<li>Word search (SEARCH)

<ul>
<li>support LIKE operation</li>
<li>A new implementation of the protocol from <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=848445">Practical Techniques for Searches on Encrypted Data</a></li>
<li>Proxy

<ol>
<li>split texts to keywords</li>
<li>remove repetitions</li>
<li>randomly permute the positions of the words</li>
<li>encrypt each of the words</li>
<li>paddle each word to the same size</li>
<li>send the encrypted words to the server as a token</li>
</ol>
</li>
<li>Server can only know whether the token matches.</li>
</ul>
</li>
</ol>


<h5>3.1.2 Adjustable query-based encryption</h5>

<p>However, the query set is not always known in advance. So we need to dynamically adjust a layer of SQL-aware encryption scheme for queries at runtime. <strong>Onions of encryption</strong>: different keys for different layers of onions. The proxy will not give all keys to the server at any time.</p>

<p><img src="http://puncsky.github.com/images/cryptdb/onion.png" alt="onion encryption" /></p>

<p><img src="http://puncsky.github.com/images/cryptdb/table1.png" alt="table" /></p>

<ul>
<li>Encrypt data in one or more onions</li>
<li>Multiple onions are needed in practice</li>
<li>Onion decryption (stripe-off) happens only when operations on a column are required</li>
<li>Once a layer of decryption happens, the layer remains its new state</li>
</ul>


<p>keys for each layer are calculated by</p>

<p><img src="http://puncsky.github.com/images/cryptdb/calcKey.png" alt="keys" /></p>

<p><strong>MK is the proxy's Master Key and only intended for a single principal (user).</strong></p>

<h6>a) Read query execution</h6>

<ol>
<li>Application server issues a query

<ul>
<li>SELECT ID FROM Employees WHERE Name = ‘Alice’</li>
</ul>
</li>
<li>Proxy server anonymizes the query, and adjust encryption layers if stripe-off is needed

<ul>
<li>UPDATE Table1 SET C2-Eq = DECRYPT RND(KT1,C2,Eq,RND, C2-Eq, C2-IV)</li>
</ul>
</li>
<li>DBMS server execute the adjusted query and return the results

<ul>
<li>SELECT C1-Eq, C1-IV FROM Table1 WHERE C2-Eq = x7..d</li>
</ul>
</li>
<li>Proxy server decrypts the results and sends them to the user</li>
<li>No need to decrypt data in DBMS when a new similar type of query is issued</li>
</ol>


<h6>b) Write query execution</h6>

<ul>
<li>INSERT and UPDATE queries make the proxy encrypts data to the layer that has not yet been stripped off.</li>
<li>Addition and direct comparison should not be allowed simultaneously.

<ul>
<li>If a column is incremented and then only projected, DBMS operates on add onion.</li>
<li>If a column is used in comparisons after increasing, proxy calculates the final value and updates it once and for all.</li>
</ul>
</li>
</ul>


<p>At this time, we can defend against Threat 1 (passive DB server attacks) by not exposing any plaintext to the DBMS server while most SQL queries can still be supported.</p>

<h4>3.2 Solution to Threat 2 and Challenge 2</h4>

<p>How to deal with <em>any attacks on all servers</em> while <strong>still retaining data-sharing</strong> among users? The proxy's Master Key (MK) is not adequate for multi-users. The solution is to chain encryption keys to user passwords with the specification of key access policy among users.</p>

<h5>3.2.1 Chain encryption keys to user passwords</h5>

<ul>
<li>What does the key derives from?

<ol>
<li><em>One Single principal</em> uses Global proxy Master Key (MK).</li>
<li><em>Multiple principals</em> chain encryption keys to user passwords.</li>
</ol>
</li>
</ul>


<h6>Define <em>principal</em></h6>

<p>A principal is an entity such as a user or a group, over which it is natural to specify an access policy.</p>

<ul>
<li><em>external principals</em>: real users authenticated with their passwords</li>
<li><em>internal principals</em>: entities whose privileges are acquired through delegations</li>
</ul>


<h6>Annotate the schema with access control policy</h6>

<p>We need to support data-sharing, so the access control policy should be specified to the schema.</p>

<ul>
<li>ENC_FOR(Sensitive-Column1, PrincipalA), A has access to column1.</li>
<li>(principalA typeX) SPEAKS_FOR(principalB typeY), A has access to all keys that b has access to.</li>
</ul>


<p><img src="http://puncsky.github.com/images/cryptdb/accessControl.png" alt="Access Control Policy Annotation" /></p>

<h6>Chain encryption keys</h6>

<ul>
<li>Each principal is associated with a secret and random-generated key

<ul>
<li>symmetric key (online) + public/private key pair (off-line)</li>
<li>External principal's random-generated symmetric key and private key is encrypted with user's password and stored in <em>external_keys</em></li>
</ul>
</li>
<li>If B speaks for A, A's key is encrypted with B's key and stored in <em>access_keys</em> table</li>
<li>When a user logs in, the proxy loads all the key the user has access to.</li>
<li>When a user logs out, the proxy delete all the user's keys in the memory.</li>
</ul>


<p>At this time, threat 2 is successfully conquered while the data-sharing feature is retained.</p>

<h3>4. Implementation &amp; Evaluation</h3>

<h4>4.1 Implementation</h4>

<ul>
<li>No change to the DBMS and applications</li>
<li>Portable</li>
<li>Multi-user keys: annotations and login/logout</li>
</ul>


<h4>4.2 Evaluation</h4>

<ul>
<li>Support most queries</li>
<li>Exceptional confidentiality

<ul>
<li>Most columns at RND layer</li>
<li>Most columns at OPE analyzed were less sensitive</li>
</ul>
</li>
<li>Throughput loss 26%</li>
</ul>


<h3>5. Conclusion</h3>

<ol>
<li>First practical DBMS to process most SQL queries on encrypted data

<ul>
<li>Hide DB from DBA, outsource DB</li>
</ul>
</li>
<li>Protects data of users logged out during attack, even when all servers are compromised

<ul>
<li>Limit leakage from compromised applications</li>
</ul>
</li>
<li>Modest overhead: 26% throughput loss for TPC-C</li>
<li>No changes to DBMS (e.g., Postgres, MySQL)</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[如何阅读一篇论文？]]></title>
    <link href="http://puncsky.github.com/blog/2012/11/21/ru-he-yue-du-%5B%3F%5D-pian-lun-wen-%3F/"/>
    <updated>2012-11-21T16:15:00-05:00</updated>
    <id>http://puncsky.github.com/blog/2012/11/21/ru-he-yue-du-[?]-pian-lun-wen-?</id>
    <content type="html"><![CDATA[<p>阅读是人生的终极命题之一。人生苦短，我们需要阅读来继承前人的衣钵，我们需要笔记来对抗过往的遗忘，我们需要写作来连接自我和人世沧桑。</p>

<p>那么，<a href="http://blizzard.cs.uwaterloo.ca/keshav/home/Papers/data/07/paper-reading.pdf">如何最高效地阅读一篇论文呢？</a></p>

<p>如同任何一个学习过程，读论文的周期显然是螺旋向上的，可以划分为简单的三板斧：</p>

<!--more-->


<h3>1. 略读</h3>

<p>五到十分钟的快速略读：</p>

<ol>
<li>题目，摘要，介绍</li>
<li>各部分的标题</li>
<li>如果有数学式子，大致了解有什么样的理论基础</li>
<li>结论</li>
<li>标出引用中度过的文献</li>
</ol>


<p>此时回答下列五个C：</p>

<ol>
<li>类别Category: A measurement paper? An analysis of an existing system? A description of a research prototype?</li>
<li>背景Context: Related papers? Theoretical bases?</li>
<li>正确？Correctness: Valid assumptions?</li>
<li>贡献Contributions.</li>
<li>清楚？Clarify. Well-written?</li>
</ol>


<p>此时，可以据此进一步决定该文章是否值得打印出来细看。相应的，自己写论文的时候如果给这些部分写得不出彩，也就不会被人关注到。</p>

<h3>2. 选读</h3>

<p>一个小时的选读，就像GRE阅读那样，只读关键的主干，不要读细节，比如证据、例子什么的。批注关键点、评论、或者自己不懂的地方。细读图表，标注相关阅读文献。</p>

<p>如果此时还是不懂这篇论文在说什么，有三种选择：</p>

<ol>
<li>算了，这篇文章对我将来没帮助</li>
<li>认了，大侠过两天重新读过，不过是充点背景知识先</li>
<li>拼了，进入第三阶段</li>
</ol>


<h3>3. 重现</h3>

<p>四到五小时的重现：自己基于作者的假设会怎么解决这个问题？将自己的解决过程和作者的过程一一比较，就能够发现两者各自的优缺点。</p>

<h3>后话</h3>

<p>这三个步骤并不一定是连在一起的，通常的情况是，拿着一堆论文先走第一步，几个星期后走第二部，再几个星期或者几个月后走第三步。当然，如果是赶进度的话，就甭想这么闲了 :P</p>

<p>总而言之，想要一口吃掉一篇论文似乎是很难的事情，我们需要反复咀嚼，这也是为什么我要写不完整的阅读笔记的原因。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HadoopDB: an architectural hybrid of MapReduce and DBMS technologies for analytical workloads (VLDB '09)]]></title>
    <link href="http://puncsky.github.com/blog/2012/11/20/hadoopdb-an-architectural-hybrid-of-mapreduce-and-dbms-technologies-for-analytical-workloads-vldb-09/"/>
    <updated>2012-11-20T16:20:00-05:00</updated>
    <id>http://puncsky.github.com/blog/2012/11/20/hadoopdb-an-architectural-hybrid-of-mapreduce-and-dbms-technologies-for-analytical-workloads-vldb-09</id>
    <content type="html"><![CDATA[<p><a href="http://db.cs.yale.edu/hadoopdb/">Yale DB</a></p>

<h3>1. Problem</h3>

<p>Large analytical data management (OLAP) with commodity clusters</p>

<h3>2. Challenges</h3>

<ul>
<li>Performance and efficiency

<ul>
<li>hadoop

<ul>
<li>is not for structured data analysis</li>
<li>scale well</li>
<li>open source, without cost</li>
</ul>
</li>
</ul>
</li>
<li>scalability, fault-tolerance, and flexibility

<ul>
<li>Previous parallel db fit into tens of nodes, not thousand of nodes.</li>
<li>not scale well for failures, heterogeneous machines, performance untested</li>
</ul>
</li>
</ul>


<h3>3. Solutions</h3>

<ul>
<li>MapReduce (Hadoop) + parallel db (or single-node DBs) = HadoopDB<!--more-->

<ul>
<li>Translation layer: Hive</li>
<li>Communication layer: Hadoop</li>
<li>Database layer: PostgreSQL (or MySQL, ... JDBC)</li>
</ul>
</li>
<li><p>Components</p>

<ol>
<li>Database Connector

<ul>
<li>Interface among dbs on nodes</li>
<li>extends InputFormat class -> InputFormat Implementation lib</li>
<li>JDBC-complaint</li>
</ul>
</li>
<li>Catalog

<ul>
<li>metainformation as XML = connection para + metadata</li>
</ul>
</li>
<li>Data Loader

<ul>
<li>Global Hasher: MR, repartition raw data from HDFS to NO. of nodes</li>
<li>Local Hasher: copy from HDFS, repartition the partition into chunks</li>
<li>Better load balance than Hadoop</li>
</ul>
</li>
<li>SQL - MR - SQL (SMS) Planner

<ul>
<li>extend Hive for</li>
<li>open and low cost</li>
<li>each table stored separately in HDFS, low performance in multi-table trans.
intercept normal Hive flow in</li>
<li>update MetaStore before query execution</li>
<li>between query plan generation and MR jobs
retrieve fields, determine partition keys</li>
</ul>


<p>   traverse DAG bottom-up</p>

<p> only support filter, select, aggregation</p></li>
</ol>
</li>
<li><p>Extend performance [23] with fault tolerance and heterogeneous node exp</p>

<p>  TODO: modify the current task scheduler, connect not straggler node but the replicas.</p>

<p>  Exp on EC2, Hadoop, HadoopDB, Vertica, DBMS-X</p></li>
</ul>


<h3>4. Conclusion</h3>

<ul>
<li>HadoopDB's performance &lt; parallel db for

<ul>
<li>PostgreSQL (not column-store, not compression)</li>
<li>Hadoop and Hive are young</li>
</ul>
</li>
<li>performance, heterogeneous environ., fault tolerance, flexibility</li>
</ul>


<h3>Related Readings</h3>

<p>[23] A Comparison of Approaches to Large Scale Data Analysis</p>

<p>[6] Scope [11] Hive [24] C-store [4] What is the right way to measure scale?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reliable Client Accounting for P2P-Infrastructure Hybrids (NSDI '12)]]></title>
    <link href="http://puncsky.github.com/blog/2012/11/20/reliable-client-accounting-for-p2p-infrastructure-hybrids-nsdi-12/"/>
    <updated>2012-11-20T16:16:00-05:00</updated>
    <id>http://puncsky.github.com/blog/2012/11/20/reliable-client-accounting-for-p2p-infrastructure-hybrids-nsdi-12</id>
    <content type="html"><![CDATA[<p><a href="http://www.mpi-sws.org/~paditya/">MPI-SWS</a>, UPenn, Duke, Akamai, <a href="http://www.mpi-sws.org/~paditya/papers/rca-nsdi2012.pdf">paper</a>, <a href="https://www.usenix.org/sites/default/files/conference/protected-files/nsdi12_reliable_client_accounting_for_p2p-infrastructure_hybrids.pdf">slides</a>, <a href="https://www.usenix.org/conference/nsdi12/reliable-client-accounting-hybrid-content-distribution-networks">video</a></p>

<h2>1. Problems</h2>

<p>Hybrid (Servers with assisting peers) designs in CDNs -> malicious clients can cause significant accounting inaccuracies.</p>

<h2>2. Challenges</h2>

<p>Infrastructure can not control P2P communications by malicious clients (peers). Even if infrastructure provides signed metadata and fallback (so content can not be mishandled by peers), there are still:</p>

<ul>
<li>Affect service quality</li>
<li>Misreport P2P transfers</li>
</ul>


<p>For example, inflation attack occurred for a fake download report.</p>

<h2>3. Solutions</h2>

<p>Reliable Client Accounting (RCA)</p>

<!--more-->


<p>Clients keep logs of network activity and upload them to the infrastructure periodically. The infrastructure collects logs, verifies them, and isolate suspicious nodes.</p>

<h3>1. Record client activities reliably</h3>

<p>Tamper evident logging: log recording sending/receiving history forms hash chains. Every massages contains a signature (authenticator) from its sender (O(# of messages)). -> Later, RCA only records authenticators for clients (O(# of pairs)).</p>

<h3>2. Identify misbehaving/suspicious clients</h3>

<p>By accounting the logs, infrastructure can find clients unilaterally claim fake downloads.</p>

<p>As to malicious client software:</p>

<p>What if bad clients do not follow the above steps (e.g. do not keep logs and serve bad content)? Simplify NetSession protocol to a state machine. Rules are manually set to identify bad logs not following the them.</p>

<p>What if many clients collude to cheat? It is difficult in practice (infrastructure assigns peers), but can still be found by statistical checks.</p>

<p>As to malicious users:</p>

<p>What if a user repeatedly downloading content to drive up demand? (can be amplified by Sybil attack) Statistical checks, too.</p>

<h3>3. Handle misbehavior without affecting service quality</h3>

<p>Blacklist and quarantine bad clients.</p>

<h2>4. Conclusions</h2>

<p>Since infrastructure cannot observe P2P communication, accounting is vulnerable to malicious clients (peers) in a hybrid design of CDN, e.g. inflation attacks. So the authors advance RCA to 1) keep tamper-evident logs and set rules for fixed state machines 2) perform statistical analysis on collected logs. Malicious clients can be detected and isolated effectively by RCA. The evaluation with real world Akamai NetSession shows that overhead is reasonable &lt;= 0.5%.</p>
]]></content>
  </entry>
  
</feed>
